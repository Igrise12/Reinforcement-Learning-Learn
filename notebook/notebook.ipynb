{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb39323",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73977b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0499ceb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb518d",
   "metadata": {},
   "source": [
    "### Load ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68402772",
   "metadata": {},
   "outputs": [],
   "source": [
    "enviorment_name = \"CartPole-v1\"\n",
    "env = gym.make(enviorment_name, render_mode = 'human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61307994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igrise/Project Coding/Reinforcement Learning Projects/gymnasium_learn/.venv/lib/python3.10/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:10.0\n",
      "Episode:2 Score:27.0\n",
      "Episode:3 Score:14.0\n",
      "Episode:4 Score:19.0\n",
      "Episode:5 Score:47.0\n"
     ]
    }
   ],
   "source": [
    "episodes= 5 \n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "    print(f\"Episode:{episode} Score:{score}\")\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426bf9a9",
   "metadata": {},
   "source": [
    "### Train an RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ca9886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the directory for saving the logs\n",
    "\n",
    "log_path = os.path.join('..','Training', 'Logs')\n",
    "os.makedirs(log_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04e3ad8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Training/Logs'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8b12efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igrise/Project Coding/Reinforcement Learning Projects/gymnasium_learn/.venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(enviorment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9576052a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ../Training/Logs/PPO_3\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1203 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 977         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009232572 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.00361    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.94        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 51.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 947         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010038944 |\n",
      "|    clip_fraction        | 0.064       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.672      |\n",
      "|    explained_variance   | 0.0874      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.3        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 39.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 922         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010938333 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.633      |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.9        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    value_loss           | 53          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 914         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010405927 |\n",
      "|    clip_fraction        | 0.095       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.605      |\n",
      "|    explained_variance   | 0.253       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.2        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 63.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 899         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007873133 |\n",
      "|    clip_fraction        | 0.0898      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.605      |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25          |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 58.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 891         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005741815 |\n",
      "|    clip_fraction        | 0.0445      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.59       |\n",
      "|    explained_variance   | 0.576       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.6        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 52.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 887         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005539135 |\n",
      "|    clip_fraction        | 0.0578      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.558      |\n",
      "|    explained_variance   | 0.413       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.1        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00817    |\n",
      "|    value_loss           | 68.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052604834 |\n",
      "|    clip_fraction        | 0.0272       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.559       |\n",
      "|    explained_variance   | 0.616        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.4         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00384     |\n",
      "|    value_loss           | 50.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052169426 |\n",
      "|    clip_fraction        | 0.0264       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.556       |\n",
      "|    explained_variance   | 0.824        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.1         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00501     |\n",
      "|    value_loss           | 36.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f9e34c69000>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5205a0",
   "metadata": {},
   "source": [
    "### Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aafcb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Path = os.path.join('..','Training', 'Saved Models', 'PPO_CartPole_Model')\n",
    "os.makedirs(os.path.dirname(PPO_Path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7374db70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_Path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e356e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abe4c004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igrise/Project Coding/Reinforcement Learning Projects/gymnasium_learn/.venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(PPO_Path, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2264dbd",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c24b9e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igrise/Project Coding/Reinforcement Learning Projects/gymnasium_learn/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "/home/igrise/Project Coding/Reinforcement Learning Projects/gymnasium_learn/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:259: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(500.0), np.float64(0.0))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60efb7e3",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5df0901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, _ = model.predict(state)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "421c231b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[500.]\n",
      "Episode:2 Score:[500.]\n",
      "Episode:3 Score:[500.]\n",
      "Episode:4 Score:[500.]\n",
      "Episode:5 Score:[500.]\n"
     ]
    }
   ],
   "source": [
    "episodes= 5 \n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = model.predict(obs) # Using the trained model to predict the action\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print(f\"Episode:{episode} Score:{score}\")\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff7aeec",
   "metadata": {},
   "source": [
    "### Viewing logs in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecf75f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_logs = os.path.join(log_path, 'PPO_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c59f24d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Training/Logs/PPO_2'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d9dfb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/igrise/Project Coding/Reinforcement Learning Projects/gymnasium_learn/.venv/lib/python3.10/site-packages/tensorboard/default.py:30: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.20.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir {training_logs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ebd803",
   "metadata": {},
   "source": [
    "### Adding a callback to the training Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a9513a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b18fc306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Training/Saved Models'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = os.path.join('..','Training', 'Saved Models')\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eca3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)\n",
    "eval_callback = EvalCallback(env, callback_on_new_best=stop_callback, \n",
    "                             eval_freq=10000, \n",
    "                             best_model_save_path= save_path,\n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d655e77",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m PPO \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[43mlog_path\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log_path' is not defined"
     ]
    }
   ],
   "source": [
    "PPO = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6ab306d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ../Training/Logs/PPO_4\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1258 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1005        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004258439 |\n",
      "|    clip_fraction        | 0.0303      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.544      |\n",
      "|    explained_variance   | -0.000376   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.444       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00265    |\n",
      "|    value_loss           | 25.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 945         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004572252 |\n",
      "|    clip_fraction        | 0.0444      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.537      |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.46        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00487    |\n",
      "|    value_loss           | 5.64        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 905          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032137851 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.548       |\n",
      "|    explained_variance   | 0.371        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.26         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    value_loss           | 3.59         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igrise/Project Coding/Reinforcement Learning Projects/gymnasium_learn/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036898663 |\n",
      "|    clip_fraction        | 0.0177       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.536       |\n",
      "|    explained_variance   | 0.0119       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.19         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00274     |\n",
      "|    value_loss           | 2.32         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 500.00  is above the threshold 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f9ccd897550>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=200000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c4920",
   "metadata": {},
   "source": [
    "### Changing Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d04b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch = [dict(pi =[128, 128,128,128], vf =[128, 128,128,128])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aea84fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igrise/Project Coding/Reinforcement Learning Projects/gymnasium_learn/.venv/lib/python3.10/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n",
      "/home/igrise/Project Coding/Reinforcement Learning Projects/gymnasium_learn/.venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch': net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c963e2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igrise/Project Coding/Reinforcement Learning Projects/gymnasium_learn/.venv/lib/python3.10/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ../Training/Logs/PPO_5\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.3     |\n",
      "|    ep_rew_mean     | 21.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 47       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 26.8        |\n",
      "|    ep_rew_mean          | 26.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013530438 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.682      |\n",
      "|    explained_variance   | -0.00348    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.63        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    value_loss           | 19.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 41.9        |\n",
      "|    ep_rew_mean          | 41.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 129         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017058205 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.648      |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.7        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0367     |\n",
      "|    value_loss           | 27.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 58.5        |\n",
      "|    ep_rew_mean          | 58.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 171         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013180828 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.6        |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.4        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    value_loss           | 47.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igrise/Project Coding/Reinforcement Learning Projects/gymnasium_learn/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012591569 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.575      |\n",
      "|    explained_variance   | 0.537       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.61        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 34.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 500.00  is above the threshold 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f5c2db98fd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=200000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07b31f3",
   "metadata": {},
   "source": [
    "### Using an Alternate Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5a818",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b89ffa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd86c7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65cfd92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymnasium_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
